# Preparar los datos para el modelo de aprendizaje
Antes de comenzar a usar modelos de pronóstico supervisados tenemos que preparar los datos de origen y realizar unas tareas que nos den el control del proceso y nos permitan verificar, al finalizar, la bondad y ajuste de nuestras previsiones.

Es una buena práctica y debemos empezar siempre por ella, dividir el conjunto de datos de origen en dos subconjuntos, uno de ellos nos servirá para el entrenamiento (*train*) del modelo y el otro para la comprobación -a posteriori- (*test*). Con este sistema de división de la muestra inicial de datos evitamos el *overfitting* tan preocupante en los modelo de predicción.

Otro de los trabajos preparatorios de importancia es la factorización o agrupamiento de los datos. En casos como el modelo de *naive bayes*, el uso de valores numéricos continuos en los datos de origen genera una sobredimensión exponencial de los casos posibles, por exceso de combinatoria entre variables que desborda el modelo y puede producir errores en los pronósticos.

En estos casos es siempre recomendable factorizar, agrupar y disminuir la dimensión origen de las variables de entrada agrupando valores.

En otros casos como el algoritmo *knn*, la medición de distancia entre variables es el eje del pronóstico, por lo que es necesario escalar los datos normalizarlos para equiparar la fórmula de distancia entre las variables, como veremos en los ejemplos.

## Crear particiones de la muestra {#particiones}
El primer paso en todo análisis debe ser dividir la muestra de datos en dos conjuntos de datos uno para entrenamiento y otro para test.
Esto se puede hacer a mano, por ejemplo usando la función `sample`, o con la ayuda de algunos paquetes que llevan funciones incorporadas para las particiones de datos.

### Ejemplo de partición a mano
Usaremos la base de datos de muestra de supervivientes del Titanic que se da como tabla en `dataset`. Para ver todas las series y bases de datos disponibles en dataset escribiremos `data()`.

Titanic es una tabla que indica casos y frecuencias de cada caso, por lo que para crear la tabla completa hay que expandir la tabla origen, y repetir cada caso el número de veces que indica la columna frecuencia.

```{r lectura_datos}
# cargamos los datos
    data("Titanic")
    str(Titanic)
    class(Titanic)
# Transformamos los datos wn una dataframe
    Titanic_df=as.data.frame(Titanic)
    str(Titanic_df)

# Creamos una tabla de casos competos a partir de la frecuencia de cada uno
# Esto repite cada caso el num de veces que se ha dado 
# según la frecuencia que está en la columna Freq de la tabla.
    repetir_secuencia=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) 
# tenemos una serie con el numero de registro de la tabla original y las veces que se repite    

# Creamos una nueva tabla repitiendo los casos según el modelo anterior.
    Titanic_data=Titanic_df[repetir_secuencia,]
    
# Ya no necesitamos la columna de frecuencias  y la borramos.
    Titanic_data$Freq=NULL
    head(Titanic_data)
# Como vemos todo son factores
    str(Titanic_data)
```
En este caso buscamos dividir la tabla en dos conjuntos uno de entrenamiento con el 75% de los registros y otro de comprobación o de test con el 25% restante de los registros que nos servirá para validar el modelo.

```{r}
# -----------------------------------
# DIVISION A MANO
# contamos el num de registro de la base de datos del titanic
    nrow(Titanic_data)
# calculamos el 75%
    num_reg_entrenamiento<-as.integer(0.75*nrow(Titanic_data))
    num_reg_entrenamiento
    
# Creamos una vector de 75% de los registros aleatorio
    titanic_train <- sample(nrow(Titanic_data), num_reg_entrenamiento)

# Creamos el conjunto de registros de entrenamiento pasando ese vector a la tabla 
    d_titanic_train <- Titanic_data[titanic_train,]
    head(d_titanic_train)

# Creamos los datos de comprobación o test (notese el -)
    d_titanic_test <- Titanic_data[-titanic_train,]
    head(d_titanic_test)
```
Usando una formulación simple hemos creado dos conjuntos de muestra aleatorios excluyentes de entrenamiento y muestra.

### Ejemplo de partición con `library(caret)`

Veamos otra forma de hacerlo usando la librería `library(caret)`

```{r eval=FALSE}
library(caret)
set.seed(987654321)
# creamos un vector de particion sobre la variable Survived
# el tamaño de muestra será de 75%
trainIndex=createDataPartition(Titanic_data$Survived, p=0.75)$Resample1

d_titanic_train=Titanic_data[trainIndex, ]
d_titanic_test= Titanic_data[-trainIndex, ]
```

## Categorización de los datos origen
Existe un problema en el uso de las funciones de clasificación cuando las combinaciones posibles entre variables tienden a ser infinitas. Esto sucede cuando, por ejemplo, una de las variables es de tipo numérico, y tiene datos continuos.

La mayoría de los modelos de clasificación solo son capaces de trabajar un número limitado de categorías, y por lo tanto, es necesario agrupar los datos originales y reducir las opciones combinatorias, por lo que hay que evitar siempre el uso de varialbles continuas en los datos. Si no realizamos la reducción de categorías nos arriesgamos a obtener errores, incluso evidentes, en los pronósticos.

Un caso evidente es el uso de la función de `naive_bayes` que maneja muy mal los datos continuos, pues está pensado para variables categorizadas.

La solución es realizar una categorización previa de los datos que evite el problema y a la vez simplifique el modelo de pronóstico. Categorizar significa agrupar los datos de las variables continuas en categorías próximas, simplificando las salidas y reduciendo las combinaciones.

Un ejemplo claro es redondear las salidas numéricas a números divisibles por 10 o por 5, o sustituir la variable por los cuantiles más representativos. 

Para transformar las variables y categorizarlas podemos usar varias funiciones de R como:

  - convertir a factor con la funcion `as.factor()`.
        * Las categorías de un factor se ven con la función `levels()`
        * los nombres de esas categorias los damos con la funcion `lables()`
  - la funcion `table(tabla_1$hora)` cuenta y resumen los datos
  - las funciones `quantile()` o `cut()` ayudan a dividir y categorizar variables continuas
  - fundiones de redondeo

Vamos a ver varios ejemplo con la dataset "women" que contiene las alturas y pesos de mujeres americanas de edad entre 30 y 39 años. Ambos datos de altura y peso son continuos y toman 15 posibles valores, por lo que las combinaciones cruzadas dan muchísimos casos posibles. Para simplificar las combinatorias vamos a categorizar los datos de varias maneras, como ejemplos: 

```{r }
# cargamos los datos de
    data("women")
    str(women)
    summary(women)

    length(unique(women$height)) # numero de registros unicos
    length(unique(women$weight))

# opcion 1. creamos una funcion de redondeo
    redondea5<-function(x,base=5){
            as.integer(base*round(x/base))
    }
    
# Copiamos la tabla con el nombre nuevo
    mujeres_a<-women
# pasamos los datos a sistema internacional
    mujeres_a$peso<-mujeres_a$weight*0.453592 # paso a kg
    mujeres_a$peso<- redondea5(mujeres_a$peso,10)
    length(unique(mujeres_a$peso))  
    
    mujeres_a$altura<-mujeres_a$height*2.54 # paso a cm
    mujeres_a$altura<- redondea5(mujeres_a$altura,10)
    length(unique(mujeres_a$altura))  
    
    head(mujeres_a)
```

Con la simplificacion anterior hemos pasado de 15x15 casos combinatorios a solo 3x4.

Podríamos usar también factores para convertir los datos

```{r} 
#Ejemplo de transformacion a factor
    mujeres_a$peso1<- factor(mujeres_a$peso,
                         levels = c(50, 60, 70),
                         labels = c( "flaca","media","gordita"))
# Ejemplo 2 de trans a factor:
    mujeres_a$altura1 <- factor(mujeres_a$altura, levels = c(150,160,170,180), labels = c("Bajo","Medio","Alta", "muy alta"))
    head(mujeres_a)
```

## Manejo de NA
En todos los modelos, la existencia de registros con falta de datos o NA, anula el valor de dicha evidencia en el modelo de entrenamiento.

Una solución es completar estos casos con las funciones como `impute()` del paquete `e1071` que sustituye el NA por un valor estimado, que puede ser la media. 

En la tabla de ejemplo `donors` hay muchos datos de la edad de los clientes que faltan.

```{r}
    # Vemos cuantos datos de edad faltan.
        set.seed(333)
        datos<-data.frame(a=sample(1:10, 100,replace = T))
        datos$a[c(1,3)] <- NA
        head(datos)
    
    library(e1071)
    # Imputamos la nuevos datos estimados de edad asignando usando impute
        datos$imputed <- impute(datos)
        head(datos)
    # Otra forma es hacerlo manualmente    
        datos$imputed2<-ifelse(is.na(datos$a),5,datos$a)
        head(datos)
```
