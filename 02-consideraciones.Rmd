# Consideraciones previas
Siempre tenemos que cuidar estas cuestiones antes de realizar el modelo de pronóstico. Simplemente detallaremos dos cosas básicas, por un lado generar dos conjuntos de datos que sirvan para el entrenamiento (*train*) y para la comprobación a posteriori (*test*). Con este sistema de división de la muestra evitamos el *overfitting* tan preocupante en los modelo de predicción.

Por otro lado vamos a describir algunos ejemplo de factorización o agrupamiento de los datos. En casos como naive bayes, el uso de valores numéricos continuos en los datos de origen genera una sobredimensión de los casos posibles, un exceso de combinatoria entre variables, que desborda el modelo y puede producir errores en los pronósticos.
En estos casos es siempre recomendable factorizar, agrupar y disminuir el número de opciones de cada variable.

En otros casos como el algoritmo *knn*, la medición de distancia entre variables es el eje del pronostico, por lo que es necesario escalar los datos normalizarlos para equiparar distancias entre las variables, como veremos en los ejemplos.

## Crear particiones de la muestra {#particiones}
El primer paso en todo análisis debe ser dividir la muestra de datos en dos conjuntos de datos uno para entrenamiento y otro para test.
Esto se puede hacer a mano, por ejemplo usando la función `sample`, o con la ayuda de algunos paquetes que llevan funciones incorporadas para las particiones de datos.

### Ejemplo de partición a mano
Usaremos la base de datos de muestra de supervivientes del Titanic que se da como tabla en `dataset`. Para ver todas las series y bases de datos disponibles en dataset escribiremos `data()`.

Titanic es una tabla que indica casos y frecuencias de cada caso, por lo que para crear la tabla completa hay que expandir la tabla origen, y repetir cada caso el número de veces que indica la columna frecuencia.

```{r lectura_datos}
# cargamos los datos
    data("Titanic")
    str(Titanic)
    class(Titanic)
# Transformamos los datos wn una dataframe
    Titanic_df=as.data.frame(Titanic)
    str(Titanic_df)

# Creamos una tabla de casos competos a partir de la frecuencia de cada uno
# Esto repite cada caso el num de veces que se ha dado 
# según la frecuencia que está en la columna Freq de la tabla.
    repetir_secuencia=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) 
# tenemos una serie con el numero de registro de la tabla original y las veces que se repite    

# Creamos una nueva tabla repitiendo los casos según el modelo anterior.
    Titanic_data=Titanic_df[repetir_secuencia,]
    
# Ya no necesitamos la columna de frecuencias  y la borramos.
    Titanic_data$Freq=NULL
    head(Titanic_data)
# Como vemos todo son factores
    str(Titanic_data)
```
En este caso buscamos dividir la tabla en dos conjuntos uno de entrenamiento con el 75% de los registros y otro de comprobación o de test con el 25% restante de los registros que nos servirá para validar el modelo.

```{r}
# -----------------------------------
# DIVISION A MANO
# contamos el num de registro de la base de datos del titanic
    nrow(Titanic_data)
# calculamos el 75%
    num_reg_entrenamiento<-as.integer(0.75*nrow(Titanic_data))
    num_reg_entrenamiento
    
# Creamos una vector de 75% de los registros aleatorio
    titanic_train <- sample(nrow(Titanic_data), num_reg_entrenamiento)

# Creamos el conjunto de registros de entrenamiento pasando ese vector a la tabla 
    d_titanic_train <- Titanic_data[titanic_train,]
    head(d_titanic_train)

# Creamos los datos de comprobación o test (notese el -)
    d_titanic_test <- Titanic_data[-titanic_train,]
    head(d_titanic_test)
```
Usando una formulación simple hemos creado dos conjuntos de muestra aleatorios excluyentes de entrenamiento y muestra.

### Ejemplo de partición con `library(caret)`

Veamos otra forma de hacerlo usando la librería `library(caret)`

```{r eval=FALSE}
library(caret)
set.seed(987654321)
# creamos un vector de particion sobre la variable Survived
# el tamaño de muestra será de 75%
trainIndex=createDataPartition(Titanic_data$Survived, p=0.75)$Resample1

d_titanic_train=Titanic_data[trainIndex, ]
d_titanic_test= Titanic_data[-trainIndex, ]
```

## Categorización de los datos origen
Existe un problema en el uso de las funciones de clasificación cuando las combinaciones posibles entre variables tienden a ser infinitas. Esto sucede cuando, por ejemplo, una de las variables es de tipo numérico, y tiene datos continuos.

La mayoría de los modelos de clasificación solo son capaces de trabajar un número limitado de categorías, y por lo tanto, es necesario agrupar los datos originales y reducir las opciones combinatorias, por lo que hay que evitar siempre el uso de variables continuas en los datos. Si no realizamos la reducción de categorías nos arriesgamos a obtener errores, incluso evidentes, en los pronósticos.

Un caso evidente es el uso de la función de `naive_bayes` que maneja muy mal los datos continuos, pues está pensado para variables categorizadas.

La solución es realizar una categorización previa de los datos que evite el problema y a la vez simplifique el modelo de pronóstico. Categorizar significa agrupar los datos de las variables continuas en categorías próximas, simplificando las salidas y reduciendo las combinaciones.

Un ejemplo claro es redondear las salidas numéricas a números divisibles por 10 o por 5, o sustituir la variable por los cuantiles más representativos. 

Para transformar las variables y categorizarlas podemos usar varias funciones de R como:

  - convertir a factor con la función `as.factor()`.
        * Las categorías de un factor se ven con la función `levels()`
        * los nombres de esas categorías los damos con la función `lables()`
  - la función `table(tabla_1$hora)` cuenta y resumen los datos
  - las funciones `quantile()` o `cut()` ayudan a dividir y categorizar variables continuas
  - funciones de redondeo

Vamos a ver varios ejemplo con la dataset "women" que contiene las alturas y pesos de mujeres americanas de edad entre 30 y 39 años. Ambos datos de altura y peso son continuos y toman 15 posibles valores, por lo que las combinaciones cruzadas dan muchísimos casos posibles. Para simplificar las combinatorias vamos a categorizar los datos de varias maneras, como ejemplos: 

```{r }
# cargamos los datos de
    data("women")
    str(women)
    summary(women)

    length(unique(women$height)) # numero de registros unicos
    length(unique(women$weight))

# opcion 1. creamos una funcion de redondeo
    redondea5<-function(x,base=5){
            as.integer(base*round(x/base))
    }
    
# Copiamos la tabla con el nombre nuevo
    mujeres_a<-women
# pasamos los datos a sistema internacional
    mujeres_a$peso<-mujeres_a$weight*0.453592 # paso a kg
    mujeres_a$peso<- redondea5(mujeres_a$peso,10)
    length(unique(mujeres_a$peso))  
    
    mujeres_a$altura<-mujeres_a$height*2.54 # paso a cm
    mujeres_a$altura<- redondea5(mujeres_a$altura,10)
    length(unique(mujeres_a$altura))  
    
    head(mujeres_a)
```

Con la simplificación anterior hemos pasado de 15x15 casos combinatorios a solo 3x4.

Podríamos usar también factores para convertir los datos

```{r} 
#Ejemplo de transformacion a factor
    mujeres_a$peso1<- factor(mujeres_a$peso,
                         levels = c(50, 60, 70),
                         labels = c( "flaca","media","gordita"))
# Ejemplo 2 de trans a factor:
    mujeres_a$altura1 <- factor(mujeres_a$altura, levels = c(150,160,170,180), labels = c("Bajo","Medio","Alta", "muy alta"))
    head(mujeres_a)
```

## Manejo de NA
En todos los modelos, la existencia de registros con falta de datos o NA, anula el valor de dicha evidencia en el modelo de entrenamiento.

Una solución es completar estos casos con las funciones como `impute()` del paquete `e1071` que sustituye el NA por un valor estimado, que puede ser la media. 

En la tabla de ejemplo `donors` hay muchos datos de la edad de los clientes que faltan.

```{r}
    # Vemos cuantos datos de edad faltan.
        set.seed(333)
        datos<-data.frame(a=sample(1:10, 100,replace = T))
        datos$a[c(1,3)] <- NA
        head(datos)
    
    library(e1071)
    # Imputamos la nuevos datos estimados de edad asignando usando impute
        datos$imputed <- impute(datos)
        head(datos)
    # Otra forma es hacerlo manualmente    
        datos$imputed2<-ifelse(is.na(datos$a),5,datos$a)
        head(datos)
```
